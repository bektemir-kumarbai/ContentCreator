import os
import mimetypes
from google import genai
from google.genai import types
from config import settings
from typing import Dict, List
import json
from pathlib import Path


class GeminiService:
    def __init__(self):
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        api_key = settings.gemini_api_key or os.environ.get("GEMINI_API_KEY")
        
        if not api_key:
            raise ValueError(
                "GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω! "
                "–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ: GEMINI_API_KEY=–≤–∞—à_–∫–ª—é—á\n"
                "–ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á –º–æ–∂–Ω–æ –∑–¥–µ—Å—å: https://aistudio.google.com/apikey"
            )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç —Å API –∫–ª—é—á–æ–º
        self.client = genai.Client(api_key=api_key)
        self.text_model_name = settings.gemini_text_model
        self.image_model_name = settings.gemini_image_model
        self.chat_history = []
    
    async def rewrite_for_tts(self, original_text: str) -> str:
        """
        –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø—Ä–∏—Ç—á–∏ –¥–ª—è –æ–∑–≤—É—á–∫–∏ —Å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ —Ç–µ–≥–∞–º–∏
        """
        prompt = f"""
–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç –¥–ª—è –∞—É–¥–∏–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç –ø—Ä–∏—Ç—á–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –æ–∑–≤—É—á–∫–∏ –≥–æ–ª–æ—Å–æ–≤—ã–º —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–æ–º.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–º –∏ –¥—Ä–∞–º–∞—Ç—É—Ä–≥–∏—á–µ—Å–∫–∏–º
2. –î–æ–±–∞–≤—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏ –¥–ª—è ElevenLabs (–∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ —Ç–µ–≥–∏):

   –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ï –°–û–°–¢–û–Ø–ù–ò–Ø:
   [excited] ‚Äî –≤–æ–∑–±—É–∂–¥–µ–Ω–∏–µ, –≤–æ–ª–Ω–µ–Ω–∏–µ
   [nervous] ‚Äî –Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å, –±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ
   [frustrated] ‚Äî —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ, —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è
   [sorrowful] ‚Äî –ø–µ—á–∞–ª—å, —Å–∫–æ—Ä–±—å
   [calm] ‚Äî —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ, —É–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ

   –†–ï–ê–ö–¶–ò–ò:
   [sigh] ‚Äî –≤–∑–¥–æ—Ö
   [laughs] ‚Äî —Å–º–µ—Ö
   [gulps] ‚Äî –≥–ª–æ—Ç–∞–Ω–∏–µ (–æ—Ç –≤–æ–ª–Ω–µ–Ω–∏—è)
   [gasps] ‚Äî –∑–∞–¥—ã—Ö–∞–Ω–∏–µ, —É–¥–∏–≤–ª–µ–Ω–∏–µ
   [whispers] ‚Äî —à–µ–ø–æ—Ç

   –ö–û–ì–ù–ò–¢–ò–í–ù–´–ï –ü–ê–£–ó–´:
   [pauses] ‚Äî –ø–∞—É–∑–∞, —Ä–∞–∑–¥—É–º—å–µ
   [hesitates] ‚Äî –∫–æ–ª–µ–±–∞–Ω–∏–µ, –Ω–µ—Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
   [stammers] ‚Äî –∑–∞–∏–∫–∞–Ω–∏–µ, –∑–∞–ø–∏–Ω–∫–∞
   [resigned tone] ‚Äî —Å–º–∏—Ä–µ–Ω–Ω—ã–π —Ç–æ–Ω

   –¢–û–ù–ê–õ–¨–ù–´–ï –û–¢–¢–ï–ù–ö–ò:
   [cheerfully] ‚Äî –≤–µ—Å–µ–ª–æ, —Ä–∞–¥–æ—Å—Ç–Ω–æ
   [flatly] ‚Äî –±–µ–∑—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ
   [deadpan] ‚Äî –Ω–µ–≤–æ–∑–º—É—Ç–∏–º–æ, —Å –∫–∞–º–µ–Ω–Ω—ã–º –ª–∏—Ü–æ–º
   [playfully] ‚Äî –∏–≥—Ä–∏–≤–æ, —à—É—Ç–ª–∏–≤–æ

3. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –¥—Ä—É–≥–∏–µ —Ç–µ–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä [sad], [angry], [dramatically], [softly] –∏ —Ç.–¥.)
4. –°–æ—Ö—Ä–∞–Ω–∏ —è–∑—ã–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–Ω–µ –ø–µ—Ä–µ–≤–æ–¥–∏!)
5. –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º ‚Äî –¥–ª—è –≤–∏–¥–µ–æ –¥–æ 60 —Å–µ–∫—É–Ω–¥
6. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–π –æ–∑–≤—É—á–∫–∏
7. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏–π

–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–ö–°–¢:
{original_text}
"""
        
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=prompt),
                ],
            ),
        ]
        
        response = self.client.models.generate_content(
            model=self.text_model_name,
            contents=contents,
        )
        
        return response.text.strip()
    
    async def generate_metadata_and_prompts(self, original_text: str, tts_text: str) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è YouTube –∏ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è YouTube Shorts.

–û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–ò–¢–ß–ê:
{original_text}

–¢–ï–ö–°–¢ –î–õ–Ø –û–ó–í–£–ß–ö–ò:
{tts_text}

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–∑–¥–∞—Ç—å:

1. –ó–ê–ì–û–õ–û–í–û–ö –¥–ª—è YouTube Shorts (–¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤, —Ü–µ–ø–ª—è—é—â–∏–π, –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï)
2. –û–ü–ò–°–ê–ù–ò–ï –¥–ª—è YouTube (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï)
3. –•–≠–®–¢–ï–ì–ò (5-10 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ö—ç—à—Ç–µ–≥–æ–≤, –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï)
4. –ü–†–û–ú–ü–¢–´ –î–õ–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô (3-7 –ø—Ä–æ–º–ø—Ç–æ–≤, –ù–ê –ê–ù–ì–õ–ò–ô–°–ö–û–ú –Ø–ó–´–ö–ï)

–í–ê–ñ–ù–û:
- –ó–∞–≥–æ–ª–æ–≤–æ–∫, –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Ö—ç—à—Ç–µ–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
- –ü—Ä–æ–º–ø—Ç—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ù–ê –ê–ù–ì–õ–ò–ô–°–ö–û–ú –Ø–ó–´–ö–ï

–í–ê–ñ–ù–û –ø—Ä–æ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:
- –ö–∞–∂–¥—ã–π –ø—Ä–æ–º–ø—Ç = –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ü–µ–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏
- –ü—Ä–æ–º–ø—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏
- –û–ø–∏—Å—ã–≤–∞–π —Å—Ç–∏–ª—å: "cinematic, dramatic lighting, detailed, 4K"
- –û–ø–∏—Å—ã–≤–∞–π –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –¥–µ—Ç–∞–ª—å–Ω–æ (—á—Ç–æ–±—ã –æ–Ω–∏ –±—ã–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö)
- –§–æ—Ä–º–∞—Ç: –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã + —Å—Ç–∏–ª—å

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "youtube_title": "–∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º",
  "youtube_description": "–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º",
  "youtube_hashtags": "#—Ö—ç—à—Ç–µ–≥1 #—Ö—ç—à—Ç–µ–≥2 #—Ö—ç—à—Ç–µ–≥3",
  "image_prompts": [
    "prompt for scene 1 in English",
    "prompt for scene 2 in English",
    "prompt for scene 3 in English"
  ]
}}
"""
        
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=prompt),
                ],
            ),
        ]
        
        response = self.client.models.generate_content(
            model=self.text_model_name,
            contents=contents,
        )
        
        text = response.text.strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()
        
        return json.loads(text)
    
    async def generate_images_with_context(self, prompts: List[str], parable_id: int) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ —á–∞—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API gemini-2.5-flash-image
        –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_dir = settings.upload_dir / "images" / str(parable_id)
        image_dir.mkdir(parents=True, exist_ok=True)
        
        generated_images = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        existing_images = {}
        for idx in range(len(prompts)):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (Gemini –æ–±—ã—á–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç JPEG)
            for ext in ['.jpeg', '.jpg', '.png', '.webp']:
                image_path = image_dir / f"scene_{idx}{ext}"
                if image_path.exists():
                    existing_images[idx] = str(image_path)
                    generated_images.append(str(image_path))
                    print(f"[Image Generation] ‚úÖ Scene {idx + 1} already exists: {image_path}")
                    break
        
        # –ï—Å–ª–∏ –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É–∂–µ –µ—Å—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö
        if len(existing_images) == len(prompts):
            print(f"[Image Generation] ‚úÖ All {len(prompts)} images already generated")
            return generated_images
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        print(f"[Image Generation] Need to generate {len(prompts) - len(existing_images)} images")
        
        # –í–ê–ñ–ù–û: –°–æ–∑–¥–∞—ë–º –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤—Å–µ—Ö —Å—Ü–µ–Ω
        story_context = f"""You are creating a visual story with {len(prompts)} connected scenes.

CRITICAL REQUIREMENTS FOR CONSISTENCY:
1. SAME ART STYLE: Use identical artistic style, rendering technique, and visual quality across all scenes
2. SAME CHARACTERS: If characters appear, they must have the EXACT same face, body, clothing, and appearance
3. SAME COLOR PALETTE: Maintain consistent color grading, saturation, and mood
4. SAME LIGHTING: Keep similar lighting conditions and atmosphere
5. SAME LEVEL OF DETAIL: Maintain consistent quality and detail level
6. VERTICAL FORMAT: Always 9:16 ratio for YouTube Shorts

Think of this as frames from the same movie - everything must look like it belongs together."""
        
        for idx, prompt in enumerate(prompts):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            if idx in existing_images:
                continue
            
            print(f"[Image Generation] Generating scene {idx + 1}/{len(prompts)}...")
            print(f"[Image Generation] Prompt: {prompt[:100]}...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ü–µ–Ω
            scene_context = f"Scene {idx + 1} of {len(prompts)}"
            if idx > 0:
                scene_context += f"\n\nPREVIOUS SCENES CONTEXT:"
                scene_context += f"\n- You have already created {idx} scene(s) before this one"
                scene_context += f"\n- This scene MUST match the style and characters from previous scenes"
                scene_context += f"\n- Continue the visual narrative smoothly and consistently"
            
            full_prompt = f"""{story_context}

{scene_context}

SCENE DESCRIPTION:
{prompt}

STYLE: Cinematic, realistic, dramatic lighting, high quality, vertical 9:16 format."""
            
            # –°–æ–∑–¥–∞—ë–º –ù–û–í–´–ô –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏)
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=full_prompt)]
                )
            ]
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            # –í–ê–ñ–ù–û: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∑–∞–ø—è—Ç—ã–µ –∏–∑ JSON
            generate_content_config = types.GenerateContentConfig(
                response_modalities=["IMAGE", "TEXT"],
                image_config=types.ImageConfig(
                    aspect_ratio="9:16"  # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è YouTube Shorts
                )
            )
            
            image_saved = False
            text_parts = []
            
            try:
                for chunk in self.client.models.generate_content_stream(
                    model=self.image_model_name,
                    contents=contents,  # –ü–µ—Ä–µ–¥–∞—ë–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
                    config=generate_content_config
                ):
                    if (
                        chunk.candidates is None
                        or not chunk.candidates
                        or chunk.candidates[0].content is None
                        or chunk.candidates[0].content.parts is None
                    ):
                        continue
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
                    for part in chunk.candidates[0].content.parts:
                        # –°–ù–ê–ß–ê–õ–ê –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if hasattr(part, 'inline_data') and part.inline_data:
                            if hasattr(part.inline_data, 'data') and part.inline_data.data and not image_saved:
                                inline_data = part.inline_data
                                data_buffer = inline_data.data
                                mime_type = inline_data.mime_type if hasattr(inline_data, 'mime_type') else 'image/jpeg'
                                
                                print(f"[Image Generation] üé® Found image data! mime_type: {mime_type}")
                                print(f"[Image Generation] Data type: {type(data_buffer)}")
                                print(f"[Image Generation] Data length: {len(data_buffer)}")
                                
                                # –í–ê–ñ–ù–û: –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64
                                # –î–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å str –∏–ª–∏ bytes, –Ω–æ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ —ç—Ç–æ base64
                                import base64
                                try:
                                    # –ï—Å–ª–∏ —ç—Ç–æ bytes, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ str –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                                    if isinstance(data_buffer, bytes):
                                        data_buffer = data_buffer.decode('utf-8')
                                        print(f"[Image Generation] Converted bytes to str")
                                    
                                    # –¢–µ–ø–µ—Ä—å –¥–µ–∫–æ–¥–∏—Ä—É–µ–º base64
                                    print(f"[Image Generation] Decoding base64 data (length: {len(data_buffer)})...")
                                    data_buffer = base64.b64decode(data_buffer)
                                    print(f"[Image Generation] ‚úÖ Decoded to {len(data_buffer)} bytes")
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                    if len(data_buffer) < 100:
                                        print(f"[Image Generation] ‚ùå Data too small, not an image!")
                                        continue
                                        
                                except Exception as e:
                                    print(f"[Image Generation] ‚ùå Base64 decode error: {e}")
                                    print(f"[Image Generation] First 100 chars: {str(data_buffer)[:100]}")
                                    continue
                                
                                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏–∑ mime_type
                                file_extension = mimetypes.guess_extension(mime_type)
                                if not file_extension:
                                    # Fallback: –µ—Å–ª–∏ mime_type –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω
                                    if 'jpeg' in mime_type.lower() or 'jpg' in mime_type.lower():
                                        file_extension = '.jpeg'
                                    elif 'png' in mime_type.lower():
                                        file_extension = '.png'
                                    elif 'webp' in mime_type.lower():
                                        file_extension = '.webp'
                                    else:
                                        file_extension = '.jpeg'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é JPEG
                                
                                print(f"[Image Generation] Extension: {file_extension}, Size: {len(data_buffer)} bytes")
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                file_name = f"scene_{idx}{file_extension}"
                                image_path = image_dir / file_name
                                
                                with open(image_path, "wb") as f:
                                    f.write(data_buffer)
                                
                                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –Ω—É–∂–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                                while len(generated_images) <= idx:
                                    generated_images.append(None)
                                generated_images[idx] = str(image_path)
                                
                                image_saved = True
                                print(f"[Image Generation] ‚úÖ Scene {idx + 1} saved: {image_path}")
                        
                        # –ü–û–¢–û–ú —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                        if hasattr(part, 'text') and part.text:
                            text_parts.append(part.text)
                
                # –ï—Å–ª–∏ –±—ã–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞—Å—Ç–∏, –≤—ã–≤–æ–¥–∏–º –∏—Ö
                if text_parts and not image_saved:
                    full_text = ''.join(text_parts)
                    print(f"[Image Generation] Model text response: {full_text[:200]}...")
                    print(f"[Image Generation] ‚ö†Ô∏è  No image data received, only text!")
                
                if not image_saved:
                    print(f"[Image Generation] ‚ö†Ô∏è  Warning: No image generated for scene {idx + 1}")
                    # –î–æ–±–∞–≤–ª—è–µ–º None —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫
                    while len(generated_images) <= idx:
                        generated_images.append(None)
                    generated_images[idx] = None
                    
            except Exception as e:
                print(f"[Image Generation] ‚ùå Error generating scene {idx + 1}: {str(e)}")
                # –î–æ–±–∞–≤–ª—è–µ–º None —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫
                while len(generated_images) <= idx:
                    generated_images.append(None)
                generated_images[idx] = None
                continue
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ
        successful_images = [img for img in generated_images if img is not None]
        print(f"\n[Image Generation] ‚úÖ Generated {len(successful_images)}/{len(prompts)} images")
        return successful_images
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ENGLISH TRANSLATION METHODS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    async def translate_to_english_for_tts(self, russian_text: str) -> str:
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –¥–ª—è –æ–∑–≤—É—á–∫–∏
        """
        prompt = f"""
You are a professional translator and scriptwriter for audio content.

Your task: Translate Russian text to English specifically for voice-over by text-to-speech synthesizer.

REQUIREMENTS:
1. Make the text expressive and dramatic
2. Add emotional tags for ElevenLabs (use ONLY these tags):

   EMOTIONAL STATES:
   [excited] ‚Äî excitement, agitation
   [nervous] ‚Äî nervousness, anxiety
   [frustrated] ‚Äî disappointment, frustration
   [sorrowful] ‚Äî sadness, sorrow
   [calm] ‚Äî calmness, peace

   REACTIONS:
   [sigh] ‚Äî sigh
   [laughs] ‚Äî laughter
   [gulps] ‚Äî gulp (from excitement)
   [gasps] ‚Äî gasp, surprise
   [whispers] ‚Äî whisper

   COGNITIVE PAUSES:
   [pauses] ‚Äî pause, reflection
   [hesitates] ‚Äî hesitation, indecision
   [stammers] ‚Äî stutter, stumble
   [resigned tone] ‚Äî resigned tone

   TONAL NUANCES:
   [cheerfully] ‚Äî cheerfully, joyfully
   [flatly] ‚Äî emotionlessly, monotonously
   [deadpan] ‚Äî impassively, deadpan
   [playfully] ‚Äî playfully, jokingly

3. DO NOT use other tags (e.g. [sad], [angry], [dramatically], [softly], etc.)
4. Keep the text short ‚Äî for videos up to 60 seconds
5. Use short sentences for better voice-over
6. Return ONLY the translated text, without headings or explanations

RUSSIAN TEXT:
{russian_text}
"""
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=prompt),
                ],
            ),
        ]
        
        response = self.client.models.generate_content(
            model=self.text_model_name,
            contents=contents,
        )
        
        return response.text.strip()
    
    async def generate_english_metadata_and_prompts(self, russian_tts_text: str, english_tts_text: str) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è YouTube –∏ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        prompt = f"""
You are an expert in creating content for YouTube Shorts.

RUSSIAN TTS TEXT (for context):
{russian_tts_text}

ENGLISH TTS TEXT:
{english_tts_text}

Your task ‚Äî create:

1. TITLE for YouTube Shorts (up to 100 characters, catchy, IN ENGLISH)
2. DESCRIPTION for YouTube (2-3 sentences, IN ENGLISH)
3. HASHTAGS (5-10 relevant hashtags, IN ENGLISH)
4. IMAGE GENERATION PROMPTS (3-7 prompts, IN ENGLISH)

IMPORTANT:
- Title, description, and hashtags must be IN ENGLISH
- Image prompts must be IN ENGLISH

IMPORTANT about image prompts:
- Each prompt = separate story scene
- Prompts should be sequential and connected
- Describe style: "cinematic, dramatic lighting, detailed, 4K"
- Describe characters in detail (so they are the same in all images)
- Format: short scene description + style

Return result STRICTLY in JSON format:
{{
  "youtube_title": "title in English",
  "youtube_description": "description in English",
  "youtube_hashtags": "#hashtag1 #hashtag2 #hashtag3",
  "image_prompts": [
    "prompt for scene 1 in English",
    "prompt for scene 2 in English",
    "prompt for scene 3 in English"
  ]
}}
"""
        
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=prompt),
                ],
            ),
        ]
        
        response = self.client.models.generate_content(
            model=self.text_model_name,
            contents=contents,
        )
        
        text = response.text.strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        if "```json" in text:
            json_start = text.find("```json") + 7
            json_end = text.find("```", json_start)
            json_text = text[json_start:json_end].strip()
        elif "```" in text:
            json_start = text.find("```") + 3
            json_end = text.find("```", json_start)
            json_text = text[json_start:json_end].strip()
        else:
            json_text = text
        
        try:
            result = json.loads(json_text)
            return result
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON: {e}")
            print(f"Response text: {text}")
            raise ValueError(f"Failed to parse JSON response from Gemini: {e}")
    

